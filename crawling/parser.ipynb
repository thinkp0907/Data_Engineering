{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parser.py\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "\n",
    "## python파일의 위치\n",
    "BASE_DIR = os.path.dirname(os.path.abspath('C:/Users/Think/data_engineer/crawling/base_dir'))\n",
    "\n",
    "req = requests.get('https://beomi.github.io/beomi.github.io_old/')\n",
    "html = req.text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "## CSS Selector를 통해 html 요소들을 찾아낸다.\n",
    "my_titles = soup.select(\n",
    "    'h3 > a'\n",
    ")\n",
    "\n",
    "data = {}\n",
    "\n",
    "for title in my_titles:\n",
    "    data[title.text] = title.get('href')\n",
    "    \n",
    "with open(os.path.join(BASE_DIR, 'result.json'), 'w+') as json_file:\n",
    "    json.dump(data, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'taske', 'pw': '!q2w3e4r5t'}\n",
      "200\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# parser.py\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "# 로그인할 유저정보를 넣어줍시다. (모두 문자열입니다!)\n",
    "LOGIN_INFO = {\n",
    "    \"id\": 'taske',\n",
    "    \"pw\": '!q2w3e4r5t'\n",
    "}\n",
    "\n",
    "# Session 생성, with 구문 안에서 유지\n",
    "with requests.Session() as s:\n",
    "    # 우선 클리앙 홈페이지에 들어가 봅시다.\n",
    "    first_page = s.get(\"https://nid.naver.com/nidlogin.login?mode=form&url=https%3A%2F%2Fwww.naver.com\")\n",
    "    html = first_page.text\n",
    "    soup = bs(html, 'html.parser')\n",
    "#     csrf = soup.find('input', {'name': '_csrf'}) # input 태그중에 name이 _csrf인 것을 찾는다\n",
    "#     print(csrf['value']) # 위에서 찾은 태그의 value를 가져옵니다.\n",
    "    \n",
    "    # 이제 LOGIN_INFO에 csrf값을 넣어줍시다.\n",
    "    # (p.s.)Python3에서 두 dict를 합치는 방법은 {**dict1, **dict2} 으로 dict들을 unpacking하는 것입니다.\n",
    "#     LOGIN_INFO = {**LOGIN_INFO, **{'_csrf': csrf['value']}}\n",
    "    print(LOGIN_INFO)\n",
    "    \n",
    "    # 이제 다시 로그인을 해봅시다.\n",
    "    login_req = s.post(\"https://nid.naver.com/nidlogin.login?mode=form&url=https%3A%2F%2Fwww.naver.com\", data=LOGIN_INFO)\n",
    "    # 어떤 결과가 나올까요? (200이면 성공!)\n",
    "    print(login_req.status_code)\n",
    "    # 로그인이 되지 않으면 경고를 띄워줍시다.\n",
    "    if login_req.status_code != 200:\n",
    "        raise Exception('로그인이 되지 않았어요! 아이디와 비밀번호를 다시한번 확인해 주세요')\n",
    "        \n",
    "    # -- 여기서부터는 로그인이 된 세션이 유지됩니다 --\n",
    "    # 이제 장터의 게시글 하나를 가져와 봅시다. 아래 예제 링크는 중고장터 공지글입니다.\n",
    "    post_one = s.get('https://section.cafe.naver.com/')\n",
    "    soup = bs(post_one.text, 'html.parser')\n",
    "    # 아래 CSS Selector는 공지글 제목을 콕 하고 집어줍니다.\n",
    "    title = soup.select('#content > div.home_editors > div.common_title')\n",
    "#     content = soup.select('#app > div > div > div.ArticleContentBox > div.article_container > div.article_viewer > div > div.content.CafeViewer > div > div')\n",
    "    # HTML을 제대로 파싱한 뒤에는 .text속성을 이용합니다.\n",
    "    print(title) # 글제목의 문자만을 가져와봅시다.\n",
    "    # [0] 을하는 이유는 select로 하나만 가져와도 title자체는 리스트이기 때문입니다.\n",
    "    # 즉, 제목 글자는 title이라는 리스트의 0번(첫번째)에 들어가 있습니다.\n",
    "#     print(content[0].text) # 글내용도 마찬가지겟지요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
