# 빅데이터 적재

## 대용량 로그 파일 적재

### 빅데이터 적재 개요

수집한 데이터는 특징에 따라 처리 방식과 적재 위치가 달라질 수 있다. 크게는 **데이터 발생 주기에 따라** `일괄 배치성 데이터` 인지, `실시간 스트림 데이터`인지를 판단해야 하고, 데이터의 형식에 따라 가공 처리나 사전 검증 작업을 할 것인지도 판단해야 한다.

적재한 데이터를 **어떤 비즈니스 요건에서 활용하느냐**에 따라 적재 대상 위치가 달라질 수 있다.

![image-20210411223419893](https://github.com/thinkp0907/Data_Engineering/blob/main/BigData_Skills/img/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%20%EC%A0%81%EC%9E%AC%20%EC%A0%80%EC%9E%A5%EC%86%8C%20%EC%9C%A0%ED%98%95.PNG)

파일럿 환경에서도 스마트카의 대용량 로그 파일(스마트카 상태 정보)을 적재할 때와 실시간 로그 파일(스마트카 운전자 정보)을 적재할 때의 아키텍처와 적재 기술들이 달라진다.



## 빅데이터 적재에 활용하는 기술

### 하둡

#### 하둡 소개

하둡은 너무나도 잘 알려진 빅데이터의 핵심 소프트웨어다. 빅데이터의 에코시스템들은 대부분 하둡을 위해 존재하고 하둡에 의존해서 발전해 가고 있다 해도 과언이 아니다. 하둡은 크게 두 가지 기능이 있다. 첫 번째가 **대용량 데이터를 분산 저장하는 것이고**, 두 번째는 **분산 저장된 데이터를 가공/분석 처리하는 기능**이다.



#### 하둡의 맵리듀스

분산 병렬 처리에서의 핵심은 여러 컴퓨터에 분산 저장돼 있는 데이터로부터 어떻게 효율적으로 일을 나눠서(Map)실행시킬 수 있느냐고, 다음으로 여러 컴퓨터가 나눠서 실행한 결과들을 어떻게 하나로 모으냐(Reduce)는 것이다. 이를 쉽고 편리하게 지원하는 프레임워크가 하둡의 맵리듀스(MapReduce)다. 

![image-20210411225512938](https://github.com/thinkp0907/Data_Engineering/blob/main/BigData_Skills/img/MapReduce%20%EA%B8%B0%EB%B3%B8%20%EA%B5%AC%EC%A1%B0.PNG)

> 1. 고객정보가 담긴 1GB의 파일을 100MB 파일 10개로 나눠서 **10대의 서버(하둡 데이터노드)에 분산 저장**(나눠진 100MB 파일을 블록 파일이라 부르며, 일반적으로 128MB 블록 단위로 처리
> 2. 전체 고객정보에서 VIP 고객의 평균연봉 조회 쿼리를 실행, **10대의 서버에 분산 저장된 100MB의 고객정보 파일로부터 Map 프로그램이 각각 생성**
> 3. 실행된 Map 프로그램은 100MB의 고객정보 파일에서 **VIP고객 정보만 추출한 후, 작아진 파일(2~8MB) 크기로 Server-11(Reduce)로 전송**
> 4. Server-11에서 Reduce 프로그램이 실행되어 Server-01(Map01) ~ Server-10(Map02)이 **전송한 VIP 고객정보를 머지(50MB)해 평균을 구하고 결과 파일(1KB)을 생성**

위 과정은 대용량 데이터에 대한 처리를 여러 대의 서버들이 나누어 작업함으로써 한 대의 고성능 서버가 처리하기 힘든 작업을 신속하게 처리한다. 실제 MapReduce가 동작할 때는 한 대의 서버에 **여러 개의 블록 파일이 저장되고다 하며, 여러 개의 Map/Reduce가 한 서버에서 동시 다발적으로 실행**되기도 한다. MapReduce 프로그램에서는 내부적으론 Split, Spill, Sort, Partition, Fetch, Shuffle, Merge 등 다양한 메커니즘 들이 작동하며 이 과정을 잘 이해하고 있어야 분산 환경에서 발생하는 다양한 문제에 빠르게 대처할 수 있다.



| 주요 구성 요소 | 설명                                                         |
| -------------- | ------------------------------------------------------------ |
| DataNode       | 블록(64MB or 128MB 등) 단위로 분활된 대용량 파일들이 DataNode의 디스크에 저장 및 관리 |
| NameNode       | DataNode에 저장된 파일들의 메타 정보를 메모리상에서 로드 해서 관리 |
| EditsLog       | 파일들의 변경 이력(수정, 삭제 등) 정보가 저장되는 로그 파일  |
| FsImage        | NameNode의 메모리상에 올라와 있는 메타 정보를 스냅샵 이미지로 만들어 생성한 파일 |

[하둡 기본 요소]



#### 하둡 아키텍처

하둡은 현재 2021년 4월 기준 3.2.2 버전 Release가 된 상태이다. 하둡의 아키텍처는 1.x 버전에서 2.x 버저으로 넘어오면서 변화와 혁신을 가져왔고, 3.x가 릴리스 되면서 다시 한 번 큰 변화를 예고하였다. 먼저 1.x의 아키텍처를 알아보자.



클라인트에서 하둡에 파일을 읽기/쓰기를 할때는 우선 **NameNode를 참조해서 파일을 읽기/쓰기할 DataNode정보를 전달** 받는다. 클라이언트는 해당 정보를 이용해 DataNode에 직접 연결해서 파일을 읽기/쓰기한다. 하둡에 적재된 데이터를 **분석해야 할 때는 클라이언트가 JobTracker에게 맵리듀스 실행을 요청**하게 되며, **JobTracker가 스케줄링 정책에 따라 작업할 DataNode/TastTracker를 선정**한다.



하지만 이와 같은 하둡 1.x의 아키텍처에는 몇 가지 문제점이 있었는데, 그중 하나가 **NameNode의 이중화 기능 미지원**으로 SPOF(SIngle Point Of Failure, 단일 장애 접정)가 존재 한다는 점이다. 하둡의 파일을 적재/관리하기 위해서는 NameNode를 참조하는데, **NameNode에 문제가 발생하면 하둡 클러스터 전체에 장애**가 발생했던 것이다. 또한 분산 병렬 처리를 위한 맵리듀스를 실행할 때도 **잡 스케줄링과 리소스 배분 정책이 효율적이지 못해 병목**이 자주 발생했다.

![image-20210411231805218](https://github.com/thinkp0907/Data_Engineering/blob/main/BigData_Skills/img/%ED%95%98%EB%91%A1%201.x%20%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98.PNG)

**[하둡 1.x 아키텍처]**



하둡 2.x 아키텍처에는 1.x의 문제점을 개선하기 위한 다양한 컴포넌트 교체 및 추가됐다. 우선 클라이언트가 DataNode로부터 파일을 읽고 쓰기 전에 NameNode를 참조하게 되는데, 이때 NameNode가 1.x 버전 때와 다르게 **Active/Standby로 이중화돼 있음**을 알 수 있다. 또한 NameNode의 메모리에서 관리되는 파일들의 **네임스페이스 정보를 주기적으로 관리하기 위해 JournalNode가 추가**됐고 **주키퍼**까지 사용됐다.



무엇보다 하둡 2.x에서 가장 큰 변화는 1.x의 맵리듀스를 처리하던 `JobTracker`, `TaskTracker`를 대신해서 **Resource Manager**, **Node Manager** 가 생긴 것이다. Resource Manager는 **Node Manager의 리소스 현황들을 종합적으로 수집**해가며 작업 실행을 위한 **최적의 DataNode를 찾아** 줘서 **효율적인 잡 스케줄링**이 가능해졌고 DataNode의 **리소스 불균형 현상 문제**도 해결했다.



또한 NodeManager의 `Container`, `Application Master`는 기존 1.x의 맵리듀스 잡 외에도 **다양한 애플리케이션을** 하둡의 **DataNode에서 실행 및 관리할 수 있게 확장됐다**. 이렇게 변화된 하둡 2.x 플랫폼을 YARN(Yet Another Resource Negotiator)이라고 한다.



![image-20210411233124987](https://github.com/thinkp0907/Data_Engineering/blob/main/BigData_Skills/img/%ED%95%98%EB%91%A1%202.x%20%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98.PNG)

**[하둡 2.x 아키텍처]**



#### 하둡 활용 방안

파일럿 프로젝트에서 스마트카 상태 정보 로그는 비교적 큰 크기의 파일로서, HDFS의 특정 디렉터리에 일자 단위로 파티션해서 적재한다. 이렇게 일 단위로 분리 적재된 데이터는 일/주/월/년별로 다양한 시계열 집계 분석을 효율적으로 수행할 수 있고, 데이터를 재적재해야 하는 경우 전체 데이터가 아닌 해당 파티션의 데이터만 재적재할 수 있다는 장점이 있다.



![image-20210411234956756](https://github.com/thinkp0907/Data_Engineering/blob/main/BigData_Skills/img/%ED%8C%8C%EC%9D%BC%EB%9F%BF%20%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8%EC%97%90%EC%84%9C%EC%9D%98%20%ED%95%98%EB%91%A1%20%ED%99%9C%EC%9A%A9%20%EB%B0%A9%EC%95%88.PNG)

